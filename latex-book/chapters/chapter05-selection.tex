\chapter{Selection Methods in Genetic Algorithms}

\section{Introduction to Selection}

\subsection{Definition and Function of Selection Operator}
Selection is the process of choosing individuals from the population to become parents in the crossover process. This process determines which individuals will produce offspring in the next generation and how many offspring will be produced.

The main goal of selection is to give greater opportunities to individuals with higher fitness to be selected, with the hope that the offspring produced will also have higher fitness. Thus, selection functions as a mechanism that drives Genetic Algorithms toward optimal solutions through improving population quality from generation to generation.

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{figures/buku_ajar_page_16.png}
\caption{Basic selection process in Genetic Algorithms}
\label{fig:selection_basic_process}
\end{figure}

\subsection{Selection Pressure}
Selection pressure is a measure of how much individuals with higher fitness are favored for selection compared to other individuals. The higher the selection pressure, the greater the chance that the best individuals are selected. This pressure plays an important role because it drives Genetic Algorithms to continuously improve the average fitness of the population from one generation to the next.

The magnitude of selection pressure greatly affects the convergence rate of Genetic Algorithms. If the pressure is high, Genetic Algorithms will reach solutions faster, but there is a risk of stopping too quickly at a wrong solution (premature convergence). Conversely, if the pressure is too low, the evolution process becomes slow because the best individuals are not spread enough.

Therefore, balance is needed. Selection must be strong enough to accelerate the search for solutions, but also maintain population diversity so that variation in the population is not lost. In this way, Genetic Algorithms can avoid premature convergence and still have the opportunity to find optimal or near-optimal solutions.

\subsection{Types of Selection Operators}
The main types of selection operators include:
\begin{enumerate}
    \item Fitness Proportionate Selection (FPS)
    \begin{itemize}
        \item Roulette Wheel Selection
        \item Baker's SUS (Stochastic Universal Sampling)
    \end{itemize}
    \item Rank-Based Selection
    \item Tournament Selection
\end{enumerate}

\section{Selection Pressure}
Selection pressure is the degree to which better individuals are favored. It affects:
\begin{itemize}
    \item \textbf{High pressure}: Fast convergence but risk of premature convergence
    \item \textbf{Low pressure}: Better exploration but slower convergence
    \item \textbf{Optimal pressure}: Balance between exploration and exploitation
\end{itemize}

\section{Fitness Proportionate Selection (FPS)}

The Genetic Algorithm developed by Holland uses Fitness Proportionate Selection (FPS)~\cite{holland1975adaptation, goldberg1989genetic}, where the expected value of an individual (i.e., the expected number of times that individual will be selected for reproduction) is calculated as that individual's fitness divided by the population's average fitness.

In this method, each individual can be selected as a parent with a probability proportional to its fitness value. Therefore, individuals with higher fitness have greater opportunities to reproduce and spread their characteristics to the next generation. Thus, this method provides selection pressure on fitter individuals in the population, thus driving evolution toward better individuals over time.

\subsection{Roulette Wheel Selection}
Also known as fitness proportionate selection, individuals are selected with probability proportional to their fitness~\cite{goldberg1989genetic, obitko_selection, algorithmafternoon_selection}.

The simplest selection schema is roulette-wheel selection, also called stochastic sampling with replacement. This is a stochastic algorithm and involves the following technique:

Individuals are mapped to contiguous segments on a line, where the size of each segment is equal to that individual's fitness value. A random number is generated, and the individual whose segment spans that random number is selected. This process is repeated until the desired number of individuals is reached, called the mating population. This technique is analogous to a roulette wheel, where each slice is proportional in size to the fitness value.

\begin{table}[H]
\centering
\begin{tabular}{cccc}
\toprule
Number of Individual & Fitness Value & Selection Probability & Interval \\
\midrule
1 & 2.0 & 0.18 & [0.00, 0.18] \\
2 & 1.8 & 0.16 & [0.18, 0.34] \\
3 & 1.6 & 0.15 & [0.34, 0.49] \\
4 & 1.4 & 0.13 & [0.49, 0.62] \\
5 & 1.2 & 0.11 & [0.62, 0.73] \\
6 & 1.0 & 0.09 & [0.73, 0.82] \\
7 & 0.8 & 0.07 & [0.82, 0.89] \\
8 & 0.6 & 0.06 & [0.89, 0.95] \\
9 & 0.4 & 0.03 & [0.95, 0.98] \\
10 & 0.2 & 0.02 & [0.98, 1.00] \\
11 & 0.0 & 0.0 & -- \\
\bottomrule
\end{tabular}
\caption{Selection probability and fitness value (from Buku Ajar)}
\label{tab:selection_probability}
\end{table}

Table \ref{tab:selection_probability} shows the selection probabilities for 11 individuals, with linear ranking with selective pressure of 2, along with their fitness values. Individual 1 is the individual with the highest fitness and occupies the largest interval, while individual 10 as the individual with the second lowest fitness has the smallest interval on the line. Individual 11, with the lowest fitness, has fitness value = 0 and gets no chance for reproduction.

To select the mating population, a number of uniformly distributed random numbers (uniformly distributed between 0.0 and 1.0) are generated independently.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{figures/buku_ajar_page_18.png}
\caption{Roulette-wheel selection process with sample trials}
\label{fig:roulette_wheel_selection}
\end{figure}

The disadvantage of roulette-wheel selection is that although it provides zero bias, it does not guarantee minimum spread.

\subsubsection{Algorithm}
\begin{algorithm}
\caption{Roulette Wheel Selection}
\begin{algorithmic}
\STATE Calculate total fitness: $F = \sum_{i=1}^{N} f_i$
\STATE Generate random number: $r \sim U[0, F]$
\STATE Set cumulative fitness: $sum = 0$
\FOR{$i = 1$ to $N$}
    \STATE $sum = sum + f_i$
    \IF{$sum \geq r$}
        \STATE Select individual $i$
        \STATE \textbf{break}
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Selection Probability}
The probability of selecting individual $i$ is:
\begin{equation}
P_i = \frac{f_i}{\sum_{j=1}^{N} f_j}
\end{equation}

\subsubsection{Example}
\begin{table}[H]
\centering
\begin{tabular}{cccc}
\toprule
Individual & Fitness & Probability & Cumulative \\
\midrule
1 & 10 & 0.25 & 0.25 \\
2 & 20 & 0.50 & 0.75 \\
3 & 5 & 0.125 & 0.875 \\
4 & 5 & 0.125 & 1.0 \\
\midrule
Total & 40 & 1.0 & \\
\bottomrule
\end{tabular}
\caption{Roulette Wheel Selection Example}
\end{table}

If random number $r = 0.6$, individual 2 is selected.

\subsubsection{Advantages}
\begin{itemize}
    \item Simple to implement
    \item Fitness proportionate selection
    \item All individuals have chance of selection
\end{itemize}

\subsubsection{Disadvantages}
\begin{itemize}
    \item Premature convergence with high fitness variance
    \item Poor selection pressure with similar fitness values
    \item Problems with negative fitness values
    \item Scaling issues
\end{itemize}

\subsection{Stochastic Universal Sampling (SUS)}
Improved version of roulette wheel selection that reduces variance~\cite{baker1987reducing}.

\subsubsection{Baker's SUS}
Stochastic Universal Sampling (SUS) provides zero bias and minimum spread~\cite{baker1987reducing}. Individuals are mapped to contiguous segments on a line, where the size of each segment equals its fitness value, exactly as in roulette-wheel selection. In this method, equally spaced pointers are placed on the line equal to the number of individuals to be selected.

Let NPointer be the number of individuals to be selected, then the distance between pointers is $1/N_{Pointer}$, and the position of the first pointer is determined by a random number generated in the range $[0, 1/N_{Pointer}]$.

For example, to select 6 individuals, the distance between pointers is $1/6 = 0.167$.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{figures/buku_ajar_page_19.png}
\caption{Stochastic universal sampling with equally spaced pointers}
\label{fig:sus_selection}
\end{figure}

Stochastic universal sampling ensures offspring selection that is closer to the expected values compared to roulette-wheel selection.

\subsubsection{Algorithm}
\begin{algorithm}
\caption{Stochastic Universal Sampling}
\begin{algorithmic}
\STATE Calculate total fitness: $F = \sum_{i=1}^{N} f_i$
\STATE Calculate pointer distance: $distance = F / N$
\STATE Generate random start: $start \sim U[0, distance]$
\STATE Create pointers: $pointer_i = start + i \times distance$ for $i = 0, 1, \ldots, N-1$
\FOR{each pointer}
    \STATE Select individual using roulette wheel logic
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Advantages over Roulette Wheel}
\begin{itemize}
    \item Lower variance
    \item More uniform selection
    \item Guaranteed expected number of selections
\end{itemize}

\section{Rank-based Selection}

Rank-based selection assigns selection probabilities based on fitness rank rather than raw fitness values~\cite{grefenstette1986optimization, algorithmafternoon_ranked}.

\subsection{Overview}
Ranked-Based Selection introduces a different approach to selection in Genetic Algorithms. Instead of directly using fitness values to determine selection probability, individuals in the population are first sorted (ranked) based on their fitness values, then each individual is assigned a rank. The selection probability is then calculated based on that rank, not the actual fitness value.

This rank-based approach helps reduce problems associated with direct fitness-based selection, such as premature convergence and domination by a few very fit individuals in the early stages of the optimization process. By assigning ranks and using them for selection, Ranked-Based Selection provides more balanced and controlled selection pressure, allowing better exploration of the search space and maintaining diversity in the population.

Rankings are typically assigned linearly or exponentially, where the best individual receives the highest rank and the worst individual receives the lowest rank. Selection probability is then calculated based on that ranking using a predetermined formula or mapping function. This mapping function can be adjusted to control selection pressure, where higher pressure will favor individuals with the highest ranks, while lower pressure provides a more even distribution of selection probabilities.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/buku_ajar_page_20.png}
\caption{How the situation changes after converting fitness to order number (rank)}
\label{fig:rank_based_selection}
\end{figure}

\subsection{Linear Ranking}
\begin{equation}
P_i = \frac{1}{N} \left[ \eta^- + (\eta^+ - \eta^-) \frac{rank_i - 1}{N - 1} \right]
\end{equation}

where:
\begin{itemize}
    \item $rank_i$ is the rank of individual $i$ (1 = worst, $N$ = best)
    \item $\eta^+$ is the expected number of copies for best individual
    \item $\eta^-$ is the expected number of copies for worst individual
    \item $\eta^+ + \eta^- = 2$ (to maintain population size)
    \item Typically: $\eta^+ = 2.0$, $\eta^- = 0.0$
\end{itemize}

\subsection{Exponential Ranking}
\begin{equation}
P_i = \frac{1 - e^{-rank_i}}{c}
\end{equation}

where $c$ is a normalization constant ensuring $\sum P_i = 1$.

\subsection{Advantages of Rank Selection}
\begin{itemize}
    \item Consistent selection pressure
    \item Handles negative fitness values
    \item Prevents premature convergence
    \item Scale-independent
\end{itemize}

\subsection{Disadvantages}
\begin{itemize}
    \item Requires sorting population
    \item Loss of fitness magnitude information
    \item Computational overhead
\end{itemize}

\section{Tournament Selection}

Tournament selection randomly selects $k$ individuals and chooses the best among them~\cite{goldberg1989genetic, baeldung_tournament}.

\subsection{Overview}
Tournament selection is a strong and widely used selection mechanism in Genetic Algorithms because it can maintain a balance between diversity maintenance and selective pressure~\cite{baeldung_tournament}. Unlike roulette-wheel selection, which directly depends on an individual's fitness relative to the total population fitness, tournament selection works by holding "tournaments" among subsets of individuals, and the winner of each tournament is selected for reproduction.

The main concept of tournament selection is quite simple: instead of considering the entire population at once, a subset of individuals is randomly selected to compete with each other. The individual with the highest fitness in that "tournament" is then selected. This process is repeated until the desired number of individuals for reproduction is reached.

This method has several advantages: tournament selection maintains diversity because individuals with low fitness still have the opportunity to participate in tournaments. Additionally, this method allows selective pressure to be adjusted by setting the tournament size.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{figures/buku_ajar_page_21.png}
\caption{Tournament selection mechanism}
\label{fig:tournament_selection}
\end{figure}

\subsection{Tournament Selection Mechanism}
\begin{enumerate}
    \item Determine tournament size ($k$), i.e., the number of individuals participating in each tournament.
    \item Randomly select $k$ individuals from the population.
    \item Compare the fitness values of these individuals and select the individual with the highest fitness as the winner.
    \item Add the winner to the mating pool.
    \item Repeat steps 2â€“4 until the desired number of individuals is reached.
\end{enumerate}

\subsection{Binary Tournament}
Most common form with $k = 2$.

\begin{algorithm}
\caption{Binary Tournament Selection}
\begin{algorithmic}
\STATE Randomly select individual $i$
\STATE Randomly select individual $j$ (where $j \neq i$)
\IF{$f_i > f_j$}
    \STATE Select individual $i$
\ELSE
    \STATE Select individual $j$
\ENDIF
\end{algorithmic}
\end{algorithm}

\subsection{k-Tournament Selection}
\begin{algorithm}
\caption{k-Tournament Selection}
\begin{algorithmic}
\STATE Create empty tournament set $T$
\FOR{$i = 1$ to $k$}
    \STATE Randomly select individual and add to $T$
\ENDFOR
\STATE Select best individual from $T$
\end{algorithmic}
\end{algorithm}

\subsection{Tournament Size Effects}
\begin{itemize}
    \item $k = 1$: Random selection (no pressure)
    \item Small $k$: Low selection pressure
    \item Large $k$: High selection pressure
    \item $k = N$: Always selects best individual
\end{itemize}

\subsection{Selection Probability}
For individual with rank $r$ out of $N$ (1 = worst, $N$ = best):
\begin{equation}
P_i = \frac{1}{N} \binom{N}{k} \sum_{j=0}^{r-1} \binom{j}{k-1} \binom{N-j-1}{0}
\end{equation}

For binary tournament ($k = 2$):
\begin{equation}
P_i = \frac{2r - 1}{N^2}
\end{equation}

\subsection{Advantages}
\begin{itemize}
    \item Simple implementation
    \item No global fitness information needed
    \item Adjustable selection pressure
    \item Parallelizable
    \item Handles negative fitness values
\end{itemize}

\subsection{Disadvantages}
\begin{itemize}
    \item May select same individual multiple times
    \item Sensitive to tournament size parameter
\end{itemize}

\section{Truncation Selection}

Select the top $\mu$ individuals from population of size $\lambda$.

\subsection{Algorithm}
\begin{algorithm}
\caption{Truncation Selection}
\begin{algorithmic}
\STATE Sort population by fitness (descending)
\STATE Select top $\mu$ individuals
\STATE Create $\lambda - \mu$ offspring from selected parents
\end{algorithmic}
\end{algorithm}

\subsection{Selection Ratio}
\begin{equation}
\text{Selection ratio} = \frac{\mu}{\lambda}
\end{equation}

Common values: 0.5 (select top 50%), 0.1 (select top 10%)

\subsection{Advantages}
\begin{itemize}
    \item Simple and deterministic
    \item High selection pressure
    \item Efficient implementation
\end{itemize}

\subsection{Disadvantages}
\begin{itemize}
    \item High risk of premature convergence
    \item Loss of diversity
    \item All-or-nothing selection
\end{itemize}

\section{Boltzmann Selection}

Selection probability based on Boltzmann distribution from statistical mechanics.

\subsection{Formula}
\begin{equation}
P_i = \frac{e^{f_i/T}}{\sum_{j=1}^{N} e^{f_j/T}}
\end{equation}

where $T$ is the temperature parameter.

\subsection{Temperature Schedule}
\begin{itemize}
    \item High $T$: Nearly uniform selection (exploration)
    \item Low $T$: Strong selection pressure (exploitation)
    \item Common schedule: $T(t) = T_0 \cdot \alpha^t$ where $\alpha < 1$
\end{itemize}

\subsection{Advantages}
\begin{itemize}
    \item Adaptive selection pressure
    \item Good balance of exploration/exploitation
    \item Prevents premature convergence
\end{itemize}

\subsection{Disadvantages}
\begin{itemize}
    \item Requires temperature scheduling
    \item Computationally expensive (exponentials)
    \item Parameter tuning required
\end{itemize}

\section{Elitist Selection}

Ensures that the best individuals are preserved across generations.

\subsection{Pure Elitism}
Always copy the best individual(s) to the next generation.

\subsection{Elitist Replacement}
Replace worst individuals with best from previous generation if they're better.

\subsection{Benefits}
\begin{itemize}
    \item Guarantees monotonic improvement
    \item Prevents loss of good solutions
    \item Faster convergence to local optima
\end{itemize}

\subsection{Drawbacks}
\begin{itemize}
    \item May reduce diversity
    \item Risk of premature convergence
    \item Can slow exploration
\end{itemize}

\section{Diversity-Preserving Selection}

\subsection{Fitness Sharing}
Reduce fitness of similar individuals to maintain diversity.

\begin{equation}
f'_i = \frac{f_i}{\sum_{j=1}^{N} sh(d_{ij})}
\end{equation}

where:
\begin{equation}
sh(d) = \begin{cases}
1 - \left(\frac{d}{\sigma_{share}}\right)^\alpha & \text{if } d < \sigma_{share} \\
0 & \text{otherwise}
\end{cases}
\end{equation}

\subsection{Crowding}
Replace similar individuals in the population.

\subsection{Speciation}
Maintain multiple sub-populations (species) simultaneously.

\section{Multi-objective Selection}

For problems with multiple conflicting objectives.

\subsection{Pareto Dominance}
Individual $\mathbf{x}$ dominates $\mathbf{y}$ if:
\begin{itemize}
    \item $\mathbf{x}$ is at least as good as $\mathbf{y}$ in all objectives
    \item $\mathbf{x}$ is strictly better than $\mathbf{y}$ in at least one objective
\end{itemize}

\subsection{Non-dominated Sorting}
\begin{enumerate}
    \item Identify all non-dominated individuals (Rank 1)
    \item Remove them and find next non-dominated set (Rank 2)
    \item Continue until all individuals are ranked
\end{enumerate}

\subsection{NSGA-II Selection}
Combines non-dominated sorting with crowding distance.

\section{Selection Comparison}

\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{lccccc}
\toprule
Method & Pressure & Diversity & Complexity & Scalability & Parameters \\
\midrule
Roulette Wheel & Variable & Poor & O(N) & Poor & None \\
SUS & Variable & Good & O(N) & Poor & None \\
Rank Linear & Constant & Good & O(N log N) & Good & $\eta^+, \eta^-$ \\
Tournament & Adjustable & Good & O(1) & Excellent & $k$ \\
Truncation & High & Poor & O(N log N) & Good & $\mu/\lambda$ \\
Boltzmann & Adaptive & Excellent & O(N) & Good & $T(t)$ \\
\bottomrule
\end{tabular}
\caption{Comparison of Selection Methods}
\end{table}

\section{Selection Guidelines}

\subsection{Problem Characteristics}
\begin{itemize}
    \item \textbf{Unimodal}: High selection pressure (truncation, large tournament)
    \item \textbf{Multimodal}: Moderate pressure (binary tournament, rank selection)
    \item \textbf{Deceptive}: Low pressure with diversity preservation
\end{itemize}

\subsection{Population Size}
\begin{itemize}
    \item Small populations: Lower selection pressure
    \item Large populations: Higher pressure acceptable
\end{itemize}

\subsection{Generation Number}
\begin{itemize}
    \item Early generations: Lower pressure for exploration
    \item Later generations: Higher pressure for exploitation
\end{itemize}

\section{Hybrid Selection Strategies}

\subsection{Adaptive Selection}
Change selection method or parameters during evolution.

\subsection{Multi-level Selection}
Apply different selection at different levels (e.g., parent selection vs. survival selection).

\subsection{Combined Methods}
Use multiple selection methods simultaneously.

\section{Chapter Summary}

This chapter covered various selection methods in genetic algorithms. Selection balances exploration and exploitation, with different methods offering different selection pressures and characteristics. Tournament selection is often preferred for its simplicity and effectiveness, while rank-based methods provide consistent pressure. The choice depends on problem characteristics, population size, and desired convergence behavior.

\section{Key Concepts}
\begin{itemize}
    \item Selection pressure and its effects
    \item Proportional vs. rank-based selection
    \item Tournament selection and its variants
    \item Elitism and diversity preservation
    \item Multi-objective selection methods
    \item Guidelines for choosing selection methods
\end{itemize}