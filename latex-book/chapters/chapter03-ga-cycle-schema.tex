\chapter{GA Cycle and Holland Schema Theory}

\section{The Genetic Algorithm Cycle}
The genetic algorithm follows a cyclic process that mimics natural evolution. Understanding this cycle is crucial for implementing and analyzing GA performance.

\subsection{Detailed GA Cycle}

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=2cm, auto]
    % Define styles
    \tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!20]
    \tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!20]
    \tikzstyle{arrow} = [thick,->,>=stealth]
    
    % Nodes
    \node [process] (init) {Initialize Population};
    \node [process, below of=init] (eval) {Evaluate Fitness};
    \node [decision, below of=eval] (term) {Termination?};
    \node [process, below of=term, yshift=-1cm] (select) {Selection};
    \node [process, below of=select] (cross) {Crossover};
    \node [process, below of=cross] (mutate) {Mutation};
    \node [process, right of=mutate, xshift=3cm] (replace) {Replacement};
    \node [process] (output) at (6,-1) {Output Best};
    
    % Arrows
    \draw [arrow] (init) -- (eval);
    \draw [arrow] (eval) -- (term);
    \draw [arrow] (term) -- node {No} (select);
    \draw [arrow] (select) -- (cross);
    \draw [arrow] (cross) -- (mutate);
    \draw [arrow] (mutate) -- (replace);
    \draw [arrow] (replace) |- ([yshift=0.5cm]eval.east);
    \draw [arrow] (term) -| node [near start] {Yes} (output);
    
\end{tikzpicture}
\caption{Genetic Algorithm Cycle}
\end{figure}

\subsection{Phase 1: Initialization}
\begin{itemize}
    \item Create initial population of size $N$
    \item Generate individuals randomly or using heuristics
    \item Ensure population diversity
    \item Set generation counter $t = 0$
\end{itemize}

\subsection{Phase 2: Evaluation}
\begin{itemize}
    \item Calculate fitness for each individual
    \item Identify best and worst individuals
    \item Compute population statistics (average, variance)
\end{itemize}

\subsection{Phase 3: Termination Check}
Common termination criteria:
\begin{itemize}
    \item Maximum number of generations reached
    \item Fitness threshold achieved
    \item Population convergence (low diversity)
    \item No improvement for specified generations
    \item Maximum function evaluations reached
\end{itemize}

\subsection{Phase 4: Selection}
\begin{itemize}
    \item Choose parents for reproduction
    \item Bias selection toward fitter individuals
    \item Maintain population diversity
\end{itemize}

\subsection{Phase 5: Crossover}
\begin{itemize}
    \item Combine genetic material from selected parents
    \item Create offspring with traits from both parents
    \item Apply with probability $p_c$ (typically 0.6-0.9)
\end{itemize}

\subsection{Phase 6: Mutation}
\begin{itemize}
    \item Introduce random changes to offspring
    \item Maintain genetic diversity
    \item Apply with probability $p_m$ (typically 0.001-0.1)
\end{itemize}

\subsection{Phase 7: Replacement}
\begin{itemize}
    \item Form new population from parents and offspring
    \item Increment generation counter $t = t + 1$
    \item Return to evaluation phase
\end{itemize}

\section{Holland Schema Theory}

Schema theory, developed by John Holland~\cite{holland1975adaptation}, provides a theoretical foundation for understanding why genetic algorithms work effectively.

\subsection{What is a Schema?}
A schema is a template describing a subset of strings with similarities at certain positions~\cite{holland1975adaptation, goldberg1989genetic, course_week2}. It uses three symbols:
\begin{itemize}
    \item \textbf{0}: Fixed bit value 0
    \item \textbf{1}: Fixed bit value 1
    \item \textbf{*}: Don't care symbol (wild card)
\end{itemize}

\textbf{Example}: Schema $H = 1*0*1$ represents all 5-bit strings with:
\begin{itemize}
    \item First bit = 1
    \item Third bit = 0
    \item Fifth bit = 1
    \item Second and fourth bits can be anything
\end{itemize}

Strings matching this schema: 10001, 10011, 11001, 11011

\subsection{Schema Properties}

\subsubsection{Order of a Schema}
The order $o(H)$ is the number of fixed positions (non-* symbols):
\begin{equation}
o(H) = \text{number of defined bits in } H
\end{equation}

For $H = 1*0*1$: $o(H) = 3$

\subsubsection{Defining Length}
The defining length $\delta(H)$ is the distance between the first and last fixed positions:
\begin{equation}
\delta(H) = \text{last fixed position} - \text{first fixed position}
\end{equation}

For $H = 1*0*1$: $\delta(H) = 5 - 1 = 4$

\textbf{Examples from Buku Ajar:}
\begin{itemize}
    \item $S_1 = (* * * 0 0 1 * 1 1 0)$: $\delta(S_1) = 10 - 4 = 6$
    \item $S_2 = (* * * * 0 0 * * 0 *)$: $\delta(S_2) = 9 - 5 = 4$
    \item $S_3 = (1 1 1 0 1 * * 0 0 1)$: $\delta(S_3) = 10 - 1 = 9$
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/buku_ajar_page_8.png}
\caption{Schema characteristics: Order and Defining Length examples}
\label{fig:schema_characteristics}
\end{figure}

\subsection{Schema Theorem (Fundamental Theorem)}

The schema theorem describes how the expected number of strings matching a schema changes from generation to generation.

\subsubsection{Selection Effect}
If $m(H,t)$ is the number of strings matching schema $H$ at generation $t$, and $f(H)$ is the average fitness of strings matching $H$, then:

\begin{equation}
E[m(H,t+1)] \geq m(H,t) \cdot \frac{f(H)}{\bar{f}}
\end{equation}

where $\bar{f}$ is the average fitness of the population.

This means schemas with above-average fitness will increase in representation.

\subsubsection{Crossover Effect}
Crossover can disrupt a schema if the crossover point falls between the defining positions. The probability of schema survival is:

\begin{equation}
P_s = 1 - p_c \cdot \frac{\delta(H)}{l-1}
\end{equation}

where:
\begin{itemize}
    \item $p_c$ is the crossover probability
    \item $l$ is the string length
\end{itemize}

\subsubsection{Mutation Effect}
The probability that a schema survives mutation is:

\begin{equation}
P_m = (1 - p_m)^{o(H)}
\end{equation}

where $p_m$ is the mutation probability per bit.

\subsubsection{Combined Schema Theorem}
Combining all effects:

\begin{equation}
E[m(H,t+1)] \geq m(H,t) \cdot \frac{f(H)}{\bar{f}} \cdot \left(1 - p_c \cdot \frac{\delta(H)}{l-1}\right) \cdot (1 - p_m)^{o(H)}
\end{equation}

\subsection{Building Block Hypothesis}

The building block hypothesis states that:
\begin{itemize}
    \item Short, low-order, above-average schemas (building blocks) increase exponentially
    \item GA combines these building blocks to form optimal solutions
    \item Good solutions contain good building blocks
\end{itemize}

\subsubsection{Characteristics of Good Building Blocks}
\begin{enumerate}
    \item \textbf{Short defining length}: $\delta(H)$ is small
    \item \textbf{Low order}: $o(H)$ is small
    \item \textbf{Above-average fitness}: $f(H) > \bar{f}$
\end{enumerate}

\subsection{Role of Schema Order in Genetic Algorithms}

\subsubsection{Pattern Specificity Level}
The order of a schema indicates the level of specificity of the pattern represented in a chromosome. The higher the order, the more specific the pattern described. For example, a low-order schema like $1*****$ still represents many possible chromosomes because only one position is fixed, while a high-order schema like $101011$ is very specific and only matches one particular chromosome. Thus, order plays a role in determining how broad a schema's representation is within the population.

\subsubsection{Survival Probability in Evolution}
The order of a schema greatly influences the schema's chance of surviving through the evolution process. In genetic algorithms, two main operators that often cause changes are crossover and mutation.

Low-order schemas are relatively safer against these changes because they have few fixed positions. For example, the schema $1*****$ only locks one bit at the beginning. If mutation occurs at another position or crossover cuts through the middle, the chance of schema disruption is very small. In other words, the fewer fixed bits that must be maintained, the greater the likelihood that the schema will survive and be inherited to the next generation.

Conversely, high-order schemas like $101011$ have many fixed bits that must be exactly the same to remain valid. In such conditions, even one small mutation at one of the fixed positions can destroy the entire schema. Similarly, in the crossover process, the probability of being cut between fixed positions becomes larger. As a result, high-order schemas often quickly disappear from the population because they struggle to survive the combination of genetic variations that occur.

\subsubsection{Relationship with Selection}
Although they appear simple, low-order schemas actually play a very important role in genetic algorithms. The simplicity of this structure allows schemas to be more resistant to damage from crossover and mutation, so these patterns survive more often and are inherited to the next generation. If a simple schema contains patterns relevant to the optimal solution—for example, certain bit combinations that increase fitness value—then that schema will appear repeatedly on various chromosomes in the population.

This phenomenon aligns with the building block hypothesis put forward by Holland. Genetic algorithms do not directly search for complex solutions as a whole, but work by maintaining and arranging simple pattern blocks that have a positive contribution to fitness. These blocks are then combined through selection, crossover, and mutation, thus forming more complex genetic structures that approach the optimal solution.

The selection process plays a central role here. Individuals who have schemas with high contribution to fitness will be selected more often to reproduce. Thus, beneficial simple schemas can spread widely in the population. Over time, combinations of building blocks from various simple schemas produce solutions that are not only more complex, but also more efficient in solving problems.

\subsection{Relationship Between Holland Schema and Genome}
The number of genome sequences that can be represented by a schema depends on the number of don't care symbols (*). Each don't care symbol can have a value of 0 or 1, so a schema with $k$ don't care symbols will produce $2^k$ possible genome sequences.

Examples:
\begin{itemize}
    \item $S_4$ has 1 don't care, so there are $2^1 = 2$ possible genome sequences: 110010, 110110
    \item $S_5$ has 2 don't care, so there are $2^2 = 4$ possible genome sequences: 1110000, 1110100, 0110000, 0110100
    \item $S_6$ has 3 don't care, so there are $2^3 = 8$ possible genome sequences: 101100111, 101100110, 101100010, 101100011, 100100111, 100100110, 100100011, 100100111
\end{itemize}

\subsection{Schema Functions in Genetic Algorithms}
\begin{itemize}
    \item Provides a powerful and compact way to represent well-defined similarities among strings
    \item Serves as the theoretical basis for explaining how good (fit) patterns in the population can be maintained and multiplied through crossover and mutation
    \item Helps understand that Genetic Algorithms work not only on individual strings, but also on classes of strings that share patterns (schemas)
\end{itemize}

\section{Implicit Parallelism}

GAs process many schemas simultaneously. For a population of size $n$ with string length $l$:
\begin{itemize}
    \item Number of possible schemas: $3^l$
    \item Useful schemas processed: $O(n^3)$
\end{itemize}

This massive implicit parallelism is a key strength of GAs.

\section{Deception and Schema Theory}

\subsection{Deceptive Problems}
Problems where low-order building blocks mislead the search away from the global optimum. 

\textbf{Example}: Trap function where building blocks point toward local optima.

\subsection{Overcoming Deception}
\begin{itemize}
    \item Increase population size
    \item Use diversity-preserving techniques
    \item Apply linkage learning
    \item Use multi-objective approaches
\end{itemize}

\section{Practical Implications}

\subsection{Encoding Design}
\begin{itemize}
    \item Minimize epistasis (gene interactions)
    \item Keep related variables close together
    \item Use appropriate representation for building blocks
\end{itemize}

\subsection{Parameter Settings}
\begin{itemize}
    \item Low mutation rate to preserve building blocks
    \item Moderate crossover rate for effective recombination
    \item Sufficient population size for schema sampling
\end{itemize}

\section{Limitations of Schema Theory}

\begin{itemize}
    \item Assumes infinite population size
    \item Ignores finite population effects
    \item Doesn't account for epistasis
    \item Limited to binary representations
    \item Overlooks linkage effects
\end{itemize}

\section{Modern Extensions}

\subsection{Walsh Analysis}
Mathematical framework extending schema theory using Walsh functions.

\subsection{Fitness Landscapes}
Analysis of problem difficulty using landscape topology.

\subsection{No Free Lunch Theorem}
States that no algorithm is superior across all possible problems.

\section{Chapter Summary}

This chapter covered the genetic algorithm cycle and Holland's schema theory. The GA cycle provides a systematic approach to evolutionary search, while schema theory explains why GAs work by processing building blocks in parallel. Understanding these concepts is essential for effective GA design and application.

\section{Key Concepts}
\begin{itemize}
    \item GA cycle phases and their purposes
    \item Schema representation and properties
    \item Schema theorem and its implications
    \item Building block hypothesis
    \item Implicit parallelism in GAs
    \item Deception and its challenges
\end{itemize}